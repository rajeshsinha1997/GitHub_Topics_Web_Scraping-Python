{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6f4d66d",
   "metadata": {},
   "source": [
    "# <font color=green>GitHub Topics Web Scraping Project</font>\n",
    "\n",
    "    Date: 17/05/2022\n",
    "    Owner: Rajesh Sinha\n",
    "    \n",
    "***\n",
    "## <font color=purple>Project Description</font>\n",
    "- Extract information on all the topics available on GitHub\n",
    "- Extract information about first 1000 repositories of all topics\n",
    "\n",
    "- For every topic available,\n",
    "    - Extract topic name\n",
    "    - Extract topic description\n",
    "    - Extract total number of repositories available\n",
    "    - Extract URL of that particular topic page\n",
    "    - Save these data into a csv file\n",
    "\n",
    "- For every repository available,\n",
    "    - Extract name of the repository,\n",
    "    - Extract owner name of the repository\n",
    "    - Extract stars available on the repository\n",
    "    - Extract URL of the repository\n",
    "    - Save these datas for each topic in a csv file\n",
    "\n",
    "## <font color=purple>Technology Description</font>\n",
    "- Python\n",
    "- Jupyter Notebook\n",
    "- Selenium\n",
    "- BeautifulSoup4\n",
    "- Pandas\n",
    "\n",
    "## <font color=purple>Future Improvements</font>\n",
    "- Add multi-threading to reduce time taken to fetch data (current time ~ 6H)\n",
    "- Fetch some more public informations about the topics\n",
    "- Fetch some more public informations about the repositories\n",
    "- Fetch informations about repository owners\n",
    "\n",
    "## <font color=purple>About The Project</font>\n",
    "* __INTENTION OF THIS PROJECT IS FOR PRACTICE AND EDUCATIONAL PURPOSE ONLY__</br>\n",
    "* __ONLY THOSE INFORMATIONS HAVE BEEN COLLECTED IN THIS PROJECT WHICH ARE PUBLICLY AVAIALABLE ON [GitHub](https://github.com/topics)__</br>\n",
    "* __USAGE OF THIS PROJECT FOR ANY COMMERCIAL PURPOSE IS STRICTLY PROHIBITED__\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2524c8b6",
   "metadata": {},
   "source": [
    "## <font color=green>Install Python Libraries</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfe68ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8f249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e77a96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28802cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef03488",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afdd684",
   "metadata": {},
   "source": [
    "## <font color=green>Import Python Modules</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babc5c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "from datetime import date\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76d08dc",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299a5b44",
   "metadata": {},
   "source": [
    "## <font color=green>Open Topics Page and Save Topic URLs</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da99e8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get executable webdriver path\n",
    "_executable_webdriver_path = ChromeDriverManager().install()\n",
    "\n",
    "# function to get webdriver\n",
    "def _get_web_driver(_headless: bool=False):\n",
    "    _options = Options()\n",
    "    _options.headless=_headless\n",
    "    \n",
    "    # create webdriver with options\n",
    "    return Chrome(\n",
    "        service = Service(executable_path=_executable_webdriver_path),\n",
    "        options=_options\n",
    "    )\n",
    "        \n",
    "# funtion to check if element is visible\n",
    "def _is_element_is_visible(_wait: WebDriverWait, _by: By, _selector: str):\n",
    "    try:\n",
    "        _wait.until(method=EC.presence_of_element_located(locator=(_by, _selector)))\n",
    "        _wait.until(method=EC.visibility_of_element_located(locator=(_by, _selector)))\n",
    "        return True\n",
    "    except (NoSuchElementException, TimeoutException):\n",
    "        return False\n",
    "\n",
    "# function to click on element\n",
    "def _click_on_element(_wait: WebDriverWait, _by: By, _selector: str, _driver: Chrome):\n",
    "    try:\n",
    "        _driver.execute_script(\"arguments[0].scrollIntoView();\",_driver.find_element(by=_by, value=_selector))\n",
    "        _wait.until(EC.element_to_be_clickable(mark=(_by, _selector))).click()\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "\n",
    "# base URL\n",
    "_github_url = \"https://github.com\"\n",
    "\n",
    "# get webdriver without options\n",
    "_driver = _get_web_driver(_headless=True)\n",
    "\n",
    "# create wait\n",
    "_wait = WebDriverWait(_driver, 10)\n",
    "\n",
    "# open GitHub Topics Page\n",
    "_driver.get(url=_github_url+\"/topics\")\n",
    "_driver.maximize_window()\n",
    "\n",
    "# load full page\n",
    "_load_more_button_xpath = \".//button[contains(text(),'Load more')]\"\n",
    "while _is_element_is_visible(_wait, By.XPATH, _load_more_button_xpath):\n",
    "    _click_on_element(_wait, By.XPATH, _load_more_button_xpath, _driver)\n",
    "\n",
    "# create Beautiful Soup object\n",
    "_github_topics_page = BeautifulSoup(markup=_driver.page_source, features=\"html.parser\")\n",
    "\n",
    "# list out all topic URLs\n",
    "_topic_name_tags = _github_topics_page.find_all(name=\"a\", attrs={\"class\": \"no-underline flex-1 d-flex flex-column\"})\n",
    "_TOPIC_NAMES = [_topic_name_tag.find(name=\"p\", attrs={\"class\": \"f3 lh-condensed mb-0 mt-1 Link--primary\"}).get_text().strip() for _topic_name_tag in _topic_name_tags]\n",
    "_TOPIC_DESCRIPTIONS = [_topic_name_tag.find(name=\"p\", attrs={\"class\": \"f5 color-fg-muted mb-0 mt-1\"}).get_text().strip() for _topic_name_tag in _topic_name_tags]\n",
    "_TOPIC_URLS = [_github_url+_topic_name_tag[\"href\"] for _topic_name_tag in _topic_name_tags]\n",
    "_TOPIC_REPOSITORY_COUNTS = []\n",
    "\n",
    "print(\"FINISHED SCRAPING {} TOPICS\".format(len(_TOPIC_URLS)))\n",
    "\n",
    "# quit webdriver\n",
    "_driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33da443e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c078df",
   "metadata": {},
   "source": [
    "## <font color=green>Open Each Topic Page and Save Repository Data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f18f7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create repository data store\n",
    "_REPOSITORY_DATA = {}\n",
    "\n",
    "# fetch repository data for each topic\n",
    "for _topic_url in _TOPIC_URLS:\n",
    "    # get webdriver without options\n",
    "    _driver = _get_web_driver(_headless=True)\n",
    "\n",
    "    # create wait\n",
    "    _wait = WebDriverWait(_driver, 10)\n",
    "\n",
    "    # open each topic page\n",
    "    _driver.get(url=_topic_url)\n",
    "    _driver.maximize_window()\n",
    "\n",
    "    # load full page\n",
    "    _load_more_button_xpath = \".//button[contains(text(),'Load more')]\"\n",
    "    while _is_element_is_visible(_wait, By.XPATH, _load_more_button_xpath):\n",
    "        _click_on_element(_wait, By.XPATH, _load_more_button_xpath, _driver)\n",
    "\n",
    "    # create BeautifulSoup object\n",
    "    _github_topic_details_page = BeautifulSoup(markup=_driver.page_source, features=\"html.parser\")\n",
    "\n",
    "    # read topic name\n",
    "    _TOPIC_NAME = _github_topic_details_page.find(name=\"h1\", attrs={\"class\": \"h1\"}).get_text().strip()\n",
    "    \n",
    "    # list all repository usernnames, repository names and repository URLs\n",
    "    _name_headings = _github_topic_details_page.find_all(name=\"h3\", \n",
    "                                                        attrs={\"class\": \"f3 color-fg-muted text-normal lh-condensed\"})\n",
    "    _REPOSITORY_USERNAMES = [_name_heading.find_all(name=\"a\")[0].get_text().strip() for _name_heading in _name_headings]\n",
    "    _REPOSITORY_NAMES = [_name_heading.find_all(name=\"a\")[1].get_text().strip() for _name_heading in _name_headings]\n",
    "    _REPOSITORY_URLS = [_github_url+_name_heading.find_all(name=\"a\")[1][\"href\"].strip() for _name_heading in _name_headings]\n",
    "\n",
    "    # list all repository stars\n",
    "    _star_tags = _github_topic_details_page.find_all(name=\"span\", attrs={\"id\": \"repo-stars-counter-star\"})\n",
    "    _REPOSITORY_STARS = [_star_tag.get_text().strip() for _star_tag in _star_tags]\n",
    "    \n",
    "    # add repository count to list\n",
    "    _TOPIC_REPOSITORY_COUNTS.append(len(_REPOSITORY_NAMES))\n",
    "    \n",
    "    # add fetched data to repository data stores\n",
    "    _REPOSITORY_DATA[_TOPIC_NAME] = {\n",
    "        \"REPOSITORY USERNAME\": _REPOSITORY_USERNAMES,\n",
    "        \"REPOSITORY NAME\": _REPOSITORY_NAMES,\n",
    "        \"REPOSITORY STARS\": _REPOSITORY_STARS,\n",
    "        \"REPOSITORY URL\": _REPOSITORY_URLS\n",
    "    }\n",
    "\n",
    "    # quit driver\n",
    "    print(\"FINISHED SCRAPING DATA FOR TOPIC: {}, {} REPOSITORIES FOUND\".format(_TOPIC_NAME, len(_REPOSITORY_URLS)))\n",
    "    _driver.quit()\n",
    "\n",
    "# prepare topic data store\n",
    "_TOPIC_DATA = {\n",
    "    \"TOPIC NAME\": _TOPIC_NAMES,\n",
    "    \"TOPIC DESCRIPTION\": _TOPIC_DESCRIPTIONS,\n",
    "    \"REPOSITORY COUNT\": _TOPIC_REPOSITORY_COUNTS,\n",
    "    \"TOPIC URL\": _TOPIC_URLS\n",
    "}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2236bb",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8094152",
   "metadata": {},
   "source": [
    "## <font color=green>Save Data To CSV Files</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8e3bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base output directory path\n",
    "_base_output_directory_path = \"output/\"\n",
    "\n",
    "# check if base output directory exists\n",
    "if not os.path.exists(_base_output_directory_path):\n",
    "    # create base output directory\n",
    "    os.mkdir(_base_output_directory_path)\n",
    "\n",
    "# output directory path\n",
    "_output_directory_path=_base_output_directory_path+\"github_topics_and_repositories_scraped_data_\"+date.today().strftime(\"%d_%m_%Y\")+\"/\"\n",
    "\n",
    "# check if output directory exists\n",
    "if os.path.exists(_output_directory_path):\n",
    "    # clear output directory\n",
    "    shutil.rmtree(_output_directory_path)\n",
    "\n",
    "# create output directory by date\n",
    "os.mkdir(_output_directory_path)\n",
    "\n",
    "# save topic data\n",
    "pd.DataFrame(_TOPIC_DATA).to_csv(path_or_buf=_output_directory_path+\"topic_data.csv\", index=False)\n",
    "\n",
    "# save repository datas\n",
    "for _topic_name in _REPOSITORY_DATA:\n",
    "    pd.DataFrame(_REPOSITORY_DATA[_topic_name]).to_csv(path_or_buf=_output_directory_path+_topic_name+\"_data.csv\", index=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
